{"cells":[{"cell_type":"markdown","metadata":{"_uuid":"3f6c2bfe6b2e26c92357e896a1511195d836956e"},"source":["<center>\n","<img src=\"https://habrastorage.org/files/fd4/502/43d/fd450243dd604b81b9713213a247aa20.jpg\">\n","    \n","## [mlcourse.ai](https://mlcourse.ai) â€“ Open Machine Learning Course \n","Author: [Yury Kashnitskiy](https://yorko.github.io) (@yorko). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose."]},{"cell_type":"markdown","metadata":{"_uuid":"cb01ca96934e5c83a36a2308da9645b87a9c52a0"},"source":["## <center> Assignment 4. Sarcasm detection with logistic regression\n","    \n","We'll be using the dataset from the [paper](https://arxiv.org/abs/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https://www.kaggle.com/danofer/sarcasm).\n","\n","Sarcasm detection is easy. \n","<img src=\"https://habrastorage.org/webt/1f/0d/ta/1f0dtavsd14ncf17gbsy1cvoga4.jpeg\" />"]},{"cell_type":"code","execution_count":7,"metadata":{"_uuid":"ffa03aec57ab6150f9bec0fa56cd3a5791a3e6f4","execution":{"iopub.execute_input":"2021-09-22T20:05:20.819216Z","iopub.status.busy":"2021-09-22T20:05:20.81889Z","iopub.status.idle":"2021-09-22T20:05:22.268118Z","shell.execute_reply":"2021-09-22T20:05:22.267127Z","shell.execute_reply.started":"2021-09-22T20:05:20.819157Z"},"trusted":true},"outputs":[],"source":["# some necessary imports\n","import os\n","import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","%load_ext blackcellmagic\n","from loguru import logger\n","import snoop"]},{"cell_type":"code","execution_count":12,"metadata":{"_uuid":"b23e4fc7a1973d60e0c6da8bd60f3d921542a856","execution":{"iopub.execute_input":"2021-09-22T20:05:45.675236Z","iopub.status.busy":"2021-09-22T20:05:45.674869Z","iopub.status.idle":"2021-09-22T20:05:54.759459Z","shell.execute_reply":"2021-09-22T20:05:54.758517Z","shell.execute_reply.started":"2021-09-22T20:05:45.675174Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv('train-balanced-sarcasm.csv.zip')"]},{"cell_type":"code","execution_count":13,"metadata":{"_uuid":"4dc7b3787afa46c7eb0d0e33b0c41ab9821c4a27","execution":{"iopub.execute_input":"2021-09-22T20:05:57.108297Z","iopub.status.busy":"2021-09-22T20:05:57.107979Z","iopub.status.idle":"2021-09-22T20:05:57.153657Z","shell.execute_reply":"2021-09-22T20:05:57.152692Z","shell.execute_reply.started":"2021-09-22T20:05:57.10824Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>comment</th>\n","      <th>author</th>\n","      <th>subreddit</th>\n","      <th>score</th>\n","      <th>ups</th>\n","      <th>downs</th>\n","      <th>date</th>\n","      <th>created_utc</th>\n","      <th>parent_comment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>NC and NH.</td>\n","      <td>Trumpbart</td>\n","      <td>politics</td>\n","      <td>2</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>2016-10</td>\n","      <td>2016-10-16 23:55:23</td>\n","      <td>Yeah, I get that argument. At this point, I'd ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>You do know west teams play against west teams...</td>\n","      <td>Shbshb906</td>\n","      <td>nba</td>\n","      <td>-4</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>2016-11</td>\n","      <td>2016-11-01 00:24:10</td>\n","      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>They were underdogs earlier today, but since G...</td>\n","      <td>Creepeth</td>\n","      <td>nfl</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2016-09</td>\n","      <td>2016-09-22 21:45:37</td>\n","      <td>They're favored to win.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>This meme isn't funny none of the \"new york ni...</td>\n","      <td>icebrotha</td>\n","      <td>BlackPeopleTwitter</td>\n","      <td>-8</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>2016-10</td>\n","      <td>2016-10-18 21:03:47</td>\n","      <td>deadass don't kill my buzz</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>I could use one of those tools.</td>\n","      <td>cush2push</td>\n","      <td>MaddenUltimateTeam</td>\n","      <td>6</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>2016-12</td>\n","      <td>2016-12-30 17:00:13</td>\n","      <td>Yep can confirm I saw the tool they use for th...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label                                            comment     author  \\\n","0      0                                         NC and NH.  Trumpbart   \n","1      0  You do know west teams play against west teams...  Shbshb906   \n","2      0  They were underdogs earlier today, but since G...   Creepeth   \n","3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n","4      0                    I could use one of those tools.  cush2push   \n","\n","            subreddit  score  ups  downs     date          created_utc  \\\n","0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n","1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n","2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n","3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n","4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n","\n","                                      parent_comment  \n","0  Yeah, I get that argument. At this point, I'd ...  \n","1  The blazers and Mavericks (The wests 5 and 6 s...  \n","2                            They're favored to win.  \n","3                         deadass don't kill my buzz  \n","4  Yep can confirm I saw the tool they use for th...  "]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["train_df.head()"]},{"cell_type":"code","execution_count":14,"metadata":{"_uuid":"0a7ed9557943806c6813ad59c3d5ebdb403ffd78","execution":{"iopub.execute_input":"2021-09-22T20:06:01.549199Z","iopub.status.busy":"2021-09-22T20:06:01.548838Z","iopub.status.idle":"2021-09-22T20:06:02.343737Z","shell.execute_reply":"2021-09-22T20:06:02.342916Z","shell.execute_reply.started":"2021-09-22T20:06:01.549149Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1010826 entries, 0 to 1010825\n","Data columns (total 10 columns):\n"," #   Column          Non-Null Count    Dtype \n","---  ------          --------------    ----- \n"," 0   label           1010826 non-null  int64 \n"," 1   comment         1010773 non-null  object\n"," 2   author          1010826 non-null  object\n"," 3   subreddit       1010826 non-null  object\n"," 4   score           1010826 non-null  int64 \n"," 5   ups             1010826 non-null  int64 \n"," 6   downs           1010826 non-null  int64 \n"," 7   date            1010826 non-null  object\n"," 8   created_utc     1010826 non-null  object\n"," 9   parent_comment  1010826 non-null  object\n","dtypes: int64(4), object(6)\n","memory usage: 77.1+ MB\n"]}],"source":["train_df.info()"]},{"cell_type":"markdown","metadata":{"_uuid":"6472f52fb5ecb8bb2a6e3b292678a2042fcfe34c"},"source":["Some comments are missing, so we drop the corresponding rows."]},{"cell_type":"code","execution_count":15,"metadata":{"_uuid":"97b2d85627fcde52a506dbdd55d4d6e4c87d3f08","execution":{"iopub.execute_input":"2021-09-22T20:06:22.741214Z","iopub.status.busy":"2021-09-22T20:06:22.74074Z","iopub.status.idle":"2021-09-22T20:06:23.102818Z","shell.execute_reply":"2021-09-22T20:06:23.101932Z","shell.execute_reply.started":"2021-09-22T20:06:22.741148Z"},"trusted":true},"outputs":[],"source":["train_df.dropna(subset=['comment'], inplace=True)\n","train_df['label'].value_counts()\n","train_texts, valid_texts, y_train, y_valid = \\\n","        train_test_split(train_df['comment'], train_df['label'], random_state=17)"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"data":{"text/plain":["469600    Starting to feel pretty fucking tired of all t...\n","639137    It's like that label actually has no meaning b...\n","240293    Mained Fiora - Reworked Mained AP Tristana - W...\n","702254    Yeah lol that's right they wouldn't let black ...\n","889040              No, he made the thread asking jokingly.\n","                                ...                        \n","23988     Well the devs kind of stopped working on the o...\n","408769           totally balanced, not game breaking at all\n","688966    No amount of money can take that experience away.\n","107936    But guys, you only think D2 was good because o...\n","767875    BUT HE'S A CAREER THIRD LINER AND WE SHOULD DU...\n","Name: comment, Length: 252694, dtype: object"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["valid_texts"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from collections import defaultdict\n","from wordcloud import WordCloud, STOPWORDS"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["import string"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["def remove_punct(tokens):\n","    cleared = []\n","    for token in tokens:\n","        token = token.translate(str.maketrans(\"\", \"\", string.punctuation))\n","        cleared.append(token)\n","    return cleared"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["def generate_ngrams(text, n_gram=1):\n","    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n","    token = remove_punct(token)\n","    ngrams = zip(*[token[i:] for i in range(n_gram)])\n","    return [\" \".join(ngram) for ngram in ngrams]"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["def sarcasm_word_count():\n","    wc = defaultdict(int)\n","    sarcasms = train_df[train_df[\"label\"] == 1]\n","    for comment in sarcasms[\"comment\"]:\n","        for word in generate_ngrams(comment):\n","            wc[word] += 1\n","    return wc"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["sc = sarcasm_word_count()\n","sc = pd.DataFrame(sorted(sc.items(), key=lambda x: x[1])[::-1])\n","sc.columns = ['word', 'wordcount']"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["def not_sarcasm_word_count():\n","    wc = defaultdict(int)\n","    not_sarcasms = train_df[train_df[\"label\"] == 0]\n","    for comment in not_sarcasms[\"comment\"]:\n","        for word in generate_ngrams(comment):\n","            wc[word] += 1\n","    return wc"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["nsc = not_sarcasm_word_count()\n","nsc = pd.DataFrame(sorted(nsc.items(), key=lambda x: x[1])[::-1])\n","nsc.columns = ['word', 'wordcount']"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word</th>\n","      <th>wordcount</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td></td>\n","      <td>26402</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>one</td>\n","      <td>17918</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>think</td>\n","      <td>15384</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>people</td>\n","      <td>15245</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>will</td>\n","      <td>12546</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>147380</th>\n","      <td>yahudi</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>147381</th>\n","      <td>rfuckyouididthemath</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>147382</th>\n","      <td>kiiiiiiiil</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>147383</th>\n","      <td>preflashpoint</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>147384</th>\n","      <td>masterymasamune</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>147385 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["                       word  wordcount\n","0                                26402\n","1                       one      17918\n","2                     think      15384\n","3                    people      15245\n","4                      will      12546\n","...                     ...        ...\n","147380               yahudi          1\n","147381  rfuckyouididthemath          1\n","147382           kiiiiiiiil          1\n","147383        preflashpoint          1\n","147384      masterymasamune          1\n","\n","[147385 rows x 2 columns]"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["nsc"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["from sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.decomposition import TruncatedSVD"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n","tfidf_vec.fit_transform(train_texts.values.tolist() + valid_texts.values.tolist())\n","train_tfidf = tfidf_vec.transform(train_texts.values.tolist())\n","test_tfidf = tfidf_vec.transform(valid_texts.values.tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def runModel(train_X, train_y, test_X, test_y, test_X2):\n","    model = linear_model.LogisticRegression(C=5., solver='sag')\n","    model.fit(train_X, train_y)\n","    pred_test_y = model.predict_proba(test_X)[:,1]\n","    pred_test_y2 = model.predict_proba(test_X2)[:,1]\n","    return pred_test_y, pred_test_y2, model"]},{"cell_type":"markdown","metadata":{"_uuid":"7f0f47b98e49a185cd5cffe19fcbe28409bf00c0"},"source":["## Tasks:\n","1. Analyze the dataset, make some plots. This [Kernel](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc) might serve as an example\n","2. Build a Tf-Idf + logistic regression pipeline to predict sarcasm (`label`) based on the text of a comment on Reddit (`comment`).\n","3. Plot the words/bigrams which a most predictive of sarcasm (you can use [eli5](https://github.com/TeamHG-Memex/eli5) for that)\n","4. (optionally) add subreddits as new features to improve model performance. Apply here the Bag of Words approach, i.e. treat each subreddit as a new feature.\n","\n","## Links:\n","  - Machine learning library [Scikit-learn](https://scikit-learn.org/stable/index.html) (a.k.a. sklearn)\n","  - Kernels on [logistic regression](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-2-classification) and its applications to [text classification](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-4-more-of-logit), also a [Kernel](https://www.kaggle.com/kashnitsky/topic-6-feature-engineering-and-feature-selection) on feature engineering and feature selection\n","  - [Kaggle Kernel](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle) \"Approaching (Almost) Any NLP Problem on Kaggle\"\n","  - [ELI5](https://github.com/TeamHG-Memex/eli5) to explain model predictions"]}],"metadata":{"interpreter":{"hash":"1be9856963ee0560708bb2052661aa27357a5b7ae3c19a3b57a929c8d7be175e"},"kernelspec":{"display_name":"Python 3.8.11 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"}},"nbformat":4,"nbformat_minor":4}
