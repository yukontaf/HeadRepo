{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svYWx8G1lePZ"
   },
   "source": [
    "# Свёрточные сети\n",
    "\n",
    "Сегодня мы детальнее поговорим про общие подходы при обучении нейросетей, и что происходит внутри них.\n",
    "\n",
    "<img width='400px' src='https://raw.githubusercontent.com/sslotin/universum-dl/43390d26d5f256dcc68a6ba51998bd626b3f6d33/images/cat.png'>\n",
    "\n",
    "Если попытаться визуализировать то, что выучивает каждый нейрон в нейросети (например, посмотрев, какие входные пиксели на него сильнее всего влияют), то можно увидеть, что чем глубже находится слой, тем более абстрактные фичи он содержит.\n",
    "\n",
    "Например, в сетях для распознавания картинок первые слои учатся обнаруживать геометрические примитивы: линии, границы, углы. Следующий слой может распознавать простые геометрические фигуры. Следующий распознаёт наличие целых объектов и т. д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2YGzdiWilePg"
   },
   "source": [
    "**Weight sharing**. [Эксперименты с дропаутом](https://arxiv.org/abs/1701.05369) показывают, что в линейном слое примерно 99% весов на самом деле можно выкинуть. Логично, что в оптимальной архитектуре не должно быть бесполезных весов — лишние параметры всегда ведут к переобучению. В случае с картинками решение в том, чтобы использовать информацию о расположении пикселей относительно друг друга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8JSCr9jlePj"
   },
   "source": [
    "## Свёртки и пуллинги\n",
    "\n",
    "**Как хранятся картинки**. Когда говорят «изображение», представляйте не прямоугольник, а параллелепипед, высотой которого будет размер каналов. Например, обычные цветные RGB картинки имеют 3 канала: на красный (R), зелёный (G) и синий (B).\n",
    "\n",
    "0. Введем такую функцию, как **ядро** (англ. **kernel**) — она считает скалярное произведение вектора-входа со своим вектором-параметром.\n",
    "1. Разобьем исходный паралеллелепипед на сколько-то параллелепипедов одинакового размера вдоль размерности, соответствующей каналам. Они могут пересекаться.\n",
    "2. Каждый из них «разгладим» в вектор.\n",
    "3. К кажому такому вектору и применим по очереди каждый кернел (их обычно берут много разных).\n",
    "4. Положим то, что получилось, в новый параллелепипед.\n",
    "5. Посчитаем для кажой ячейки какую-нибудь нелинейность. Обычно это ReLU из-за вычислительных причин.\n",
    "\n",
    "<img width='350px' src='https://raw.githubusercontent.com/sslotin/universum-dl/43390d26d5f256dcc68a6ba51998bd626b3f6d33/images/conv1.png'>\n",
    "\n",
    "Эта операция называется **свёрткой**. Помимо кернела, в ней есть другие параметры — паддинг (отступ по краям), страйды (шаги по x и y). Также свёртка может быть в 2d и 3d. Посмотрите этот репозиторий, чтобы получше разобраться со свёрточной арифметикой: https://github.com/vdumoulin/conv_arithmetic\n",
    "\n",
    "![](http://deeplearning.net/software/theano/_images/numerical_padding_strides.gif)\n",
    "\n",
    "**Пулингом** называют операцию, при которой входной тензор так же разбивается на квадраты (не паралепипеды — операция независима по каждому каналу) и на каждом квадрате считается какая-нибудь редукция (чаще всего максимум или среднее по всем значениям в квадрате), после чего полученные значения записываются на следующий слой в том же порядке.\n",
    "\n",
    "<img width='400px' src='http://cs231n.github.io/assets/cnn/maxpool.jpeg'>\n",
    "\n",
    "В свёртках переиспользуется очень много параметров: кернел для каждого фильтра (выходного канала) использует один и тот же вектор-параметр для скалярного умножения. Из-за этого каждый фильтр как правило выучивает какую-то конкретную фичу, вроде наличия какого-либо объекта на своём регионе. Пулинг используют затем для понижения размерности: каждый нейрон после свертки выражает степень уверенности, что на регионе присутствует какой-то объект, и поэтому логично в качестве вероятности наличия объекта на регионе из под-регионов использовать максимум или среднее.\n",
    "\n",
    "Чаще всего используют свёртки 3x3 со страйдом 2x2 (то есть квадраты перекрываются по 3 крайним пикселям) с пулингом размера 2x2 (не перекрываются)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sIfUCzR0lePp"
   },
   "source": [
    "## Аугментация данных\n",
    "\n",
    "Аугментацией называется процесс получения новых синтетических данных из имеющихся, чтобы подать в обучение. Это часто (особенно в компьютерном зрении) позволяет улучшить качество модели, не используя дополнительных данных.\n",
    "\n",
    "Формально, в случае с классификацией, аугментация — это любое преобразование, которое корректно изменяет данные, не меняя их класс.\n",
    "\n",
    "В случае с картинками, можно попробовать добавить следующие преобразования, которые с какой-то вероятностью будут использоваться во время обучения:\n",
    "\n",
    "* Поворот на малый угол.\n",
    "* Добавление шума.\n",
    "* Обрезание границ и последующее растяжение до исходного размера.\n",
    "* Горизонтальное отражение (но в нашем случае оно вредно).\n",
    "* Смещение на небольшое расстояние.\n",
    "\n",
    "Понятно, что лейбл эти преобразования изменить не должны.\n",
    "\n",
    "<img src='https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/a5da2c3b4174449d13dd746b7d00897c6bc1f334/5-Figure2-1.png' width='500px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGVT5oqzlePr"
   },
   "source": [
    "## Взрывающийся градиент\n",
    "\n",
    "В сетях происходит очень много чего стохастического: батч формируется случайно, аугментация, рандомизированные слои вроде дропаута, вычислительные ошибки. Это всё может привести к тому, что сеть на какой-то итерации будет очень уверенна в неправильном предсказании и некоторые её параметры получат очень большой градиент. Это может привести к тому, что эти параметры «улетят» куда-то настолько далеко, и после этого сеть будет всегда предсказывать класс, который на этой итерации был правильным. \n",
    "\n",
    "Простое решение: просто обрезать градиент в случае, если градиент больше какого-то фиксированного значения. Для этого есть функция `torch.nn.utils.clip_grad_norm_`, которая принимает параметры модели и параметр `threshold`. Она считает норму (длину вектора) градиента и, если она больше `threshold`, нормирует градиенты так, чтобы она была равна `threshold`. Эта функция также возвращает само значение нормы, что может быть очень полезно при анализе обучения (например, если она становится маленькой, то, значит, сеть сходится к какому-то плато)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COWp1WKclePu",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Инициализация параметров\n",
    "\n",
    "Сначала приведем пример плохой инициализации. Пусть мы задали все значения изначально нулями. В таком случае наша модель становится эквивалентна линейной модели — производная по функции потерь одинакова для каждого $w_i$, таким образом, все веса имеют одинаковые значения и в последующей итерации, что делает нейроны в сети симметричными.\n",
    "\n",
    "Подход получше — инициализировать каждый вес случайно. Но тут нужно быть осторожным — если задать их слишком большими, то сеть может быть изначально очень уверенна в своих предсказаниях, и подвинуть параметры оттуда будет очень трудно.\n",
    "\n",
    "Решение следующее. С точки зрения слоя, ему на вход подается сэмпл из какого-то распределения, и он под это распределение подстраивается. В нейросетях размеры слоев достаточно большие, чтобы в них работали законы статистики, все разработчики фреймворках условились инициализировать веса всех слоев в предположении, что на вход подаются данные из какого-то распределения со средним 0 и дисперсией 1, и на выходе должно получиться какое-то распределение со средним тоже 0 и дисперсией 1. Чаще всего изначальные веса берут либо из нормального, либо из равномерного распределения, «обрезанного» так, чтобы дисперсия каждого выходного значения получилась единичной.\n",
    "\n",
    "Аналогично нужно поступать со входными векторными данными: нормализовывать. Это будет важно при работе с изображениями: не надо подавать на вход вектора с элементами от 0 до 255. Самое простое рабочее решение — нормализовать вход, поделив его на 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-v2sSq0elePy"
   },
   "source": [
    "## Минутка физики\n",
    "\n",
    "Потребляемая энергия в сети с переменным током в единицу времени считается по формуле\n",
    "\n",
    "$$W = CV^2f$$\n",
    "\n",
    "где $C$ означает емкость сети, $V$ означает напряжение, а $f$ — частоту. В случае с процессорами, это именно та частота, с которой выполняются элементарные операции, например сложение.\n",
    "\n",
    "<img width='350px' src='https://i.ibb.co/yhsGRJK/Screenshot-from-2019-02-08-14-52-07.png'>\n",
    "\n",
    "Однако, если мы сделаем сеть из двух параллельно подключенных процессоров, работающих на половинной частоте, мы можем получить сеть, потребляющую ~40% изначальной энергии, делающую то же количество полезной работы — то же суммарное количество процессорных тактов:\n",
    "\n",
    "<img width='450px' src='https://i.ibb.co/WgLCxxL/Screenshot-from-2019-02-08-14-52-18.png'>\n",
    "\n",
    "Поэтому для хорошо распараллеливаемых операций используют другой тип вычислительных устройств, в которых не 4-8 быстрых (3-4 GHz) процессоров, а несколько тысяч медленных (~1GHz)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eFLKQB7IleP1"
   },
   "source": [
    "## Device-agnostic код\n",
    "\n",
    "К любой модели или тензору в PyTorch можно применить `.cuda()` и `.cpu()`, чтобы перевести тензор на память GPU или в оперативную память соответственно. Но если мы будем писать такой код, нам будет довольно проблематично портировать его на другие машины, где, например, нет GPU (например, если вы хотите скачать тетрадку с colab к себе и запустить).\n",
    "\n",
    "Многие фреймворки позволяют абстрагироваться от устройства конкретных вычислительных устройств. В PyTorch для этого есть объект `torch.device`, который позволяет явно задавать, на каком устройстве хранить тензор или модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1EWLdWGFleP4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1103, -0.0238,  0.0542, -0.2294, -0.3074,  0.2601, -0.4142, -0.0545],\n",
       "        [-0.1399,  0.1796, -0.0230,  0.0137, -0.1235,  0.5387, -0.2333,  0.1891],\n",
       "        [-0.1711,  0.1226, -0.0471,  0.0629, -0.1854,  0.2366,  0.1743, -0.3125],\n",
       "        [ 0.0513, -0.0715, -0.1371,  0.0462, -0.2246,  0.2190, -0.3500, -0.0758],\n",
       "        [ 0.1894, -0.1510, -0.1577,  0.0997, -0.0116,  0.2253, -0.0196,  0.1378]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cpu:0\")  # используй первую GPU (номеруются с нуля)\n",
    "\n",
    "X = torch.randn(5, 100, device=device)  # создай матрицу на этом устройстве\n",
    "# альтернативно: X = X.to(device)\n",
    "\n",
    "# создадим какую-нибудь модель\n",
    "model = nn.Sequential(nn.Linear(100, 20), nn.ReLU(), nn.Linear(20, 8))\n",
    "\n",
    "model = model.to(device)  # переведи параметры модели на это устройство\n",
    "\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LRfKGXKGleQA"
   },
   "source": [
    "# Колоризация изображений\n",
    "\n",
    "Начнём практическую часть. Обучим autoencoder-like сеть, которая учится восстанавливать изображение по его черно-белой версии. В качестве лосса будем так же использовать какую-нибудь меру расстояния между изображениями (например, l1 или l2).\n",
    "\n",
    "Тот пайплайн, что у нас получится, с минимальными изменениями можно будет также использовать и для других подобных задач, связанных с восстановлением изображений после каких-либо необратимых преобразований, например после подмешивании шума (denoising autoencoder) или понижения размера (DeepHD).\n",
    "\n",
    "![](https://camo.githubusercontent.com/c5f95c94d70a3e52561c1d0591e84a5e3b86eb74/687474703a2f2f726963687a68616e672e6769746875622e696f2f636f6c6f72697a6174696f6e2f7265736f75726365732f696d616765732f6e65745f6469616772616d2e6a7067)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TMZ8ypgleQE"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "isyXUmOoleQM"
   },
   "source": [
    "Для начала скачаем данные. Годятся вообще любые изображения, не обязательно из какого-то изветсного датасета. Этой командой можно скачать и распакавать фотографии с одной школы по программированию, проходившей этим летом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tz3QJKqsleQO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-23 12:03:32--  http://sereja.me/f/universum_compressed.tar\n",
      "Распознаётся sereja.me (sereja.me)… 213.159.215.132\n",
      "Подключение к sereja.me (sereja.me)|213.159.215.132|:80... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 72028160 (69M) [application/x-tar]\n",
      "Сохранение в: «universum_compressed.tar»\n",
      "\n",
      "universum_compresse 100%[===================>]  68,69M  1,53MB/s    за 44s     \n",
      "\n",
      "2021-11-23 12:04:16 (1,56 MB/s) - «universum_compressed.tar» сохранён [72028160/72028160]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://sereja.me/f/universum_compressed.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xf universum_compressed.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jO34pEQmleQV"
   },
   "source": [
    "Есть два подхода к работе с данными:\n",
    "\n",
    "1. Сначала преобразовать все имеющиеся данные к виду, который принимает нейросеть (сразу к тензорам одинакового размера).\n",
    "2. Хранить сырые данные и преобразование препроцессинга (функцию) и собирать батчи на лету.\n",
    "\n",
    "Если это не что-то совсем простое, то второй вариант предпочтительнее, так как он не требует дополнительной памяти (датасеты могут быть большими), времени на векторизацию датасета, а так же сбор батча «на лету» позволяет там же делать аугментацию.\n",
    "\n",
    "Для этого в PyTorch есть две абстракции: `Dataset` и `DataLoader`.\n",
    "\n",
    "`Dataset` — абстрактный класс, от которого нужно отнаследовать класс датасета, который мы напишем. В нём должны быть определён конструктор (в нём обычно загружаются в память сырые данные, которые лежат где-то на диске, а также сохраняется какая-нибудь другая информация), метод `__len__` (должен вернуть размер датасета) и `__getitem__`, который должен по номеру сэмпла вернуть его в виде тензора (возможно, произведя какой-нибудь препроцессинг)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xfamcDfslqjA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:05<00:00,  1.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# tqdm -- это маленькая библиотечка для прорисовывания progress bar-ов прямо в питоне\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQGNRnBpleQb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, path, transform_x, transform_y):\n",
    "        self.transform_x = transform_x\n",
    "        self.transform_y = transform_y\n",
    "\n",
    "        filenames = []\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".jpg\") or file.endswith(\".JPG\"):\n",
    "                    filenames.append(os.path.join(root, file))\n",
    "\n",
    "        self.images = []\n",
    "        for filename in tqdm(filenames):\n",
    "            try:\n",
    "                with Image.open(filename) as image:\n",
    "                    self.images.append(image.copy())\n",
    "            except:\n",
    "                pass\n",
    "                # print('Could not load image:', filename)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        Y = self.transform_y(img)\n",
    "        X = self.transform_x(Y)\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bR8CNjjjleQj"
   },
   "source": [
    "Чтобы подавать картинки на вход нейросети, нужно их перевести в тензоры, причём одинакового размера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FNSJsAk4leQn"
   },
   "outputs": [],
   "source": [
    "transform_all = transforms.Compose(\n",
    "    [\n",
    "        # вырежем случайный квадратик\n",
    "        transforms.RandomResizedCrop(128),\n",
    "        # горизонтально перевернем -- изображение останется валидным\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # что бы ещё поделать, чтобы увеличить размер датасета?\n",
    "        transforms.RandomRotation(degrees=(-45, 45)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def to_grayscale(x):\n",
    "    return (x[0] * 0.299 + x[1] * 0.587 + x[2] * 0.114).view(1, 128, 128)\n",
    "    # минутка эволюционной биологии: как вы думаете, почему коэффициенты именно такие?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8EhPdNBGleQt"
   },
   "source": [
    "Здесь `transform_all` и `to_grayscale` являются функциями (формально, первый является функтором), которые мы передадим дальше в `DataLoader`, который оборачивает датасет и позволяет итерироваться по нему по батчам, а также реализует разные полезные функции вроде перемешивания данных после каждой эпохи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXNkMfkGleQx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1017/1017 [00:02<00:00, 400.30it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = ColorizationDataset(\"universum-photos\", to_grayscale, transform_all)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZMwOUGB8leQ9"
   },
   "source": [
    "**Skip-connection**. Иногда бывает полезно присоединить к выходу какого-то слоя его вход, чтобы следующий получил такую же, неизменённую копию. Здесь мы поступим именно так: подадим исходное черно-белое изображение в какую-то одну часть сети, которая сконцентрируется на определении цвета, а затем припишем последним слоем её выход и отправим дальше другому модулю, который уже раскрасит это исходное изображение. От простоты `nn.Sequential`, правда, уже придётся отказаться, и нужно написать свой класс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_pr_KE4UmXE"
   },
   "outputs": [],
   "source": [
    "class Colorizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.preconcat = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                1, 32, (3, 3), padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(\n",
    "                (2, 2), stride=(2, 2)\n",
    "            ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                32, 64, (3, 3), padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(\n",
    "                (2, 2), stride=(2, 2)\n",
    "            ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                64, 128, (3, 3), padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(\n",
    "                (2, 2), stride=(2, 2)\n",
    "            ),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                128, 256, (3, 3), padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(\n",
    "                (2, 2), stride=(2, 2)\n",
    "            ),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2),  # увеличиваем высоту и ширину в два раза\n",
    "            nn.Conv2d(256, 128, (3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),  # увеличиваем высоту и ширину в два раза\n",
    "            nn.Conv2d(256, 128, (3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            # ...\n",
    "            # много-много таких же, но наоборот\n",
    "            # ...\n",
    "        )\n",
    "\n",
    "        self.postconcat = nn.Sequential(  # эту сетку можно особо не увеличивать - она не должна быть очень умной\n",
    "            nn.Conv2d(65, 32, (3, 3), padding=1),  # подумайте, откуда у автора тут 65\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, (3, 3), padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.preconcat(x)\n",
    "        # исходное чб изображение -- просто дополнительным слоем\n",
    "        h = torch.cat((h, x), 1)\n",
    "        h = self.postconcat(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ql85BQRNleRG"
   },
   "source": [
    "Глубокие сети очень часто состоят из повторяющихся блоков, отличающихся только размерностью (в данном случае — количеством фильтров). Чтобы сократить количество кода и уменьшить вероятность багов, блоки можно обернуть в одну функцию, возвращающую мини-модель из нескольких слоев.\n",
    "\n",
    "Концептуальный пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IIf2JUzvleRK"
   },
   "outputs": [],
   "source": [
    "def Block(channels_in, channels_out):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(channels_in, channels_out, (3, 3), padding=1),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        nn.Dropout(),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(channels_in),\n",
    "    )\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Block(3, 64),\n",
    "    nn.MaxPool2d((2, 2)),\n",
    "    Block(64, 128),\n",
    "    nn.MaxPool2d((2, 2)),\n",
    "    Block(128, 256),\n",
    "    Block(256, 256),\n",
    "    Block(256, 256),\n",
    "    nn.Upsample(),\n",
    "    Block(256, 128),\n",
    "    nn.Upsample(),\n",
    "    Block(128, 64),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkR1-2rOleRP"
   },
   "source": [
    "Дальше как обычно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WUK0eH0ZleRS"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "lr = 1e-3\n",
    "\n",
    "model = Colorizer()  # .to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.L1Loss()  # тут можно поиграться с лоссами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YhDXCo2CleRc"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 256, 3, 3], expected input[64, 32, 128, 128] to have 256 channels, but got 32 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t0/fw3jvl196_v78v94hzhxhpgw0000gn/T/ipykernel_46126/3126523936.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# 2. прогнать данные через сеть\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# 3. посчитать loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/t0/fw3jvl196_v78v94hzhxhpgw0000gn/T/ipykernel_46126/3470075766.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m# исходное чб изображение -- просто дополнительным слоем\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 256, 3, 3], expected input[64, 32, 128, 128] to have 256 channels, but got 32 channels instead"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "for epoch in range(num_epochs):\n",
    "    for x, y in loader:\n",
    "        # теперь сами:\n",
    "        # 0. распакавать данные на нужное устройство\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # 1. сбросить градиент\n",
    "        optimizer.zero_grad()\n",
    "        # 2. прогнать данные через сеть\n",
    "        output = model(x)\n",
    "        # 3. посчитать loss\n",
    "        loss = criterion(output, y)\n",
    "        # 4. залоггировать его куда-нибудь\n",
    "        history.append(loss.item())\n",
    "        # 5. сделать .backward()\n",
    "        loss.backward()\n",
    "        # 6. optimizer.step()\n",
    "        optimizer.step()\n",
    "        # (7. вывести пример колоризации -- см код ниже)\n",
    "        to_numpy_image(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSNIw5CbleRf"
   },
   "source": [
    "В подобных нечётко поставленных задачах важно смотреть не цифры, а на реальные примеры.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErhISBCbnk58"
   },
   "outputs": [],
   "source": [
    "def to_numpy_image(img):\n",
    "    # tl;dr: PyTorch хочет (3, 128, 128), а plt.imshow хочет (128, 128, 3)\n",
    "    # есть два популярных формата для цветных картинок:\n",
    "    #  1. где размерность, соответствующая каналам, идёт последней\n",
    "    #  2. где размерность, соответствующая каналам, идёт первой\n",
    "    # при работе с нейросетями удобен первый подход -- так запрашиваемая\n",
    "    # при вычислениях память идёт последовательно, и из-за кэширования\n",
    "    # операции свёртки работают быстрее\n",
    "    # второй подход удобнее при уже работе с устройством, которое эти картинки показывает\n",
    "    # -- удобно на три лампочки послать сразу три последовательно идущих байта\n",
    "    return img.detach().cpu().view(3, 128, 128).transpose(0, 1).transpose(1, 2).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AASI8egleRh"
   },
   "outputs": [],
   "source": [
    "for t in range(10):\n",
    "    img_gray, img_true = dataset[t]\n",
    "    img_pred = model(img_gray.to(device).view(1, 1, 128, 128))\n",
    "    img_pred = to_numpy_image(img_pred)\n",
    "    # теперь это numpy-евский ndarray размера (128, 128, 3)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    plt.subplot(141)\n",
    "    plt.axis(\"off\")\n",
    "    plt.set_cmap(\"Greys\")\n",
    "    plt.imshow(img_gray.reshape((128, 128)))\n",
    "\n",
    "    plt.subplot(142)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img_pred.reshape((128, 128, 3)))\n",
    "\n",
    "    plt.subplot(143)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(to_numpy_image(img_true))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iR-twB6-leRs"
   },
   "source": [
    "## *Тизер: adversarial loss\n",
    "\n",
    "У нашего подхода к колоризации есть одна весьма существенная проблема: непонятно, как определять функцию потерь. Выясняется, что l1 или l2 в некоторых случаях даже являются принципиально неправильным выбором. Представьте, что у нас есть датасет фотографий с летнего лагеря, в котором все люди ходят в футболках двух разных цветов — например, красного и синего — интенсивность которых одинакова и неотличима на черно-белых версиях. Тогда наш лосс заставит сеть выбирать что-то «по середине» (в случае с l2 это будет среднее, а с l1 медиана), и, скорее всего, она сгенерирует что-то серое, в то время как она должна с какой-то вероятностью сгенерировать явно красную или явно синюю футболку.\n",
    "\n",
    "Решение в следующем: выход (колоризованное изображение) кормить в другую сеть, которая учится определять «правдоподобность» раскраски. Помимо восстановления изображения с точки зрения какой-то меры близости, сети-генератору (колоризатору) нужно ещё и обмануть сеть-дискриминатор, а сети-дискриминатору нужно наоборот, учиться отличать настоящую колоризацию от нашей.\n",
    "\n",
    "Подобные схемы с двумя состязяющимися сетями называют GAN-ам (Generative Adversarial Networks), о которых мы поговорим через занятие."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cnns_and_colorization.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
