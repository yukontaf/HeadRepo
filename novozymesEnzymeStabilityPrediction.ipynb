{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukontaf/HeadRepo/blob/main/novozymesEnzymeStabilityPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqvAfHXqxUYC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, numpy as np, matplotlib.pyplot as plt, matplotlib\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "# os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
        "from tensorflow import keras\n",
        "tf.config.run_functions_eagerly(True)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adagrad, Adadelta, Adam, Adamax, Ftrl, Nadam, SGD, RMSprop \n",
        "from tensorflow.keras.layers import Bidirectional, Dense, LSTM, GRU, Conv1D, GlobalAveragePooling1D, Lambda, Flatten, Dropout, Embedding, Input\n",
        "from tqdm import tqdm\n",
        "matplotlib.style.use(\"seaborn-whitegrid\")\n",
        "pd.set_option(\"display.width\", 5000)\n",
        "pd.set_option(\"display.max_columns\", 60)\n",
        "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "# AUTO = tf.data.experimental.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu6m1iQkcdRa"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install optuna\n",
        "!pip install blackcellmagic\n",
        "!pip install ipdb\n",
        "!apt install --allow-change-held-packages libcudnn8=8.4.1.50-1+cuda11.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCwoZ9JT4sk8"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import ipdb\n",
        "from optuna.trial import TrialState\n",
        "%load_ext blackcellmagic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUvW033axX0I",
        "outputId": "4139053d-86f1-436c-99cd-b9d9f618cf7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12ceraaz41xJ503VhZlU-WnBB5pR8QWWf\n",
            "To: /content/train.feather\n",
            "100% 12.2M/12.2M [00:00<00:00, 212MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mocZNvYWzWL9U-kygm9QU4ejuJoO0jyo\n",
            "To: /content/test.feather\n",
            "100% 45.8k/45.8k [00:00<00:00, 41.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 12ceraaz41xJ503VhZlU-WnBB5pR8QWWf\n",
        "!gdown 1mocZNvYWzWL9U-kygm9QU4ejuJoO0jyo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdJ_ise1qMPY"
      },
      "outputs": [],
      "source": [
        "df = pd.read_feather('/content/train.feather')\n",
        "df = df.drop('data_source', axis=1)\n",
        "df = df.fillna(0)\n",
        "train = df.sample(frac = 0.8)\n",
        "val = df.drop(train.index, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjpEHcvlxUYI"
      },
      "outputs": [],
      "source": [
        "def split_seq(df):\n",
        "    sentences = np.array([\n",
        "    df[['protein_sequence']].to_numpy()[:, 0],\n",
        "])\n",
        "    splitted= []\n",
        "    for i in sentences[0, :]:\n",
        "        splitted.append(list(i))\n",
        "\n",
        "    return np.array(splitted)\n",
        "\n",
        "train_seq, test_seq = split_seq(train), split_seq(val)\n",
        "\n",
        "train_ph, val_ph = train['pH'].to_numpy().reshape((len(train), 1)), val['pH'].to_numpy().reshape((len(val), 1))\n",
        "train_tm, val_tm = train['tm'].to_numpy().reshape((len(train), 1)), val['tm'].to_numpy().reshape((len(val), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ro80MZB2ZMm",
        "outputId": "7f14a7ff-cf41-4ba9-d9ee-88e826e6491f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2242.0"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "df['protein_sequence'].apply(lambda x: len(x)).quantile(0.99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4Bd-xxfxUYJ"
      },
      "outputs": [],
      "source": [
        "max_length = 2250\n",
        "trunc_type='post'\n",
        "embedding_dim = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdNq9IJfxUYJ"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_seq)\n",
        "sequences = tokenizer.texts_to_sequences(train_seq)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
        "vocab_size = len(tokenizer.word_index)\n",
        "train = tf.data.Dataset.from_tensor_slices(np.append(np.append(padded, train_ph, 1), train_tm, 1))\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(test_seq)\n",
        "testing_padded = pad_sequences(testing_sequences,maxlen=max_length)\n",
        "val = tf.data.Dataset.from_tensor_slices(np.append(np.append(testing_padded, val_ph, 1), val_tm, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo0y5tvepgqc"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "def preprocess(data):\n",
        "    sequence = data[:-1]\n",
        "    ph = tf.reshape(data[-2], (-1, ))\n",
        "    tm =  data[-1]\n",
        "    return sequence, ph, tm\n",
        "def get_training_dataset(dataset):\n",
        "    dataset = dataset.map(preprocess).shuffle(len(sequences)).batch(BATCH_SIZE)\n",
        "    return dataset\n",
        "def get_validation_dataset(valid):\n",
        "  valid = valid.map(preprocess).shuffle(len(sequences)).batch(BATCH_SIZE)\n",
        "  return valid\n",
        "def concat(input):\n",
        "  # if len(input[0].shape) >=2:\n",
        "  #   input[1] = tf.tile(tf.reshape(input[1], (-1, 1, 1)), (1, 1, input[0].shape[-1]))\n",
        "  #   return tf.keras.layers.concatenate([input[0], input[1]], axis=1)\n",
        "  # else:\n",
        "    return tf.keras.layers.concatenate([input[0], input[1]], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3JlzGcFePL-"
      },
      "outputs": [],
      "source": [
        "# %pdb\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, lstm_layers, emb_dim, lstm_units, dropout_rate):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.lstm_layers, self.emb_dim, self.lstm_units, self.dropout_rate = lstm_layers, emb_dim, lstm_units, dropout_rate\n",
        "    self.input_ph = tf.keras.layers.Input((1,))\n",
        "    self.input_seq = tf.keras.layers.Input((500, ))\n",
        "    self.seq_layers =  [] \n",
        "    self.seq_layers.extend([tf.keras.layers.Embedding(21, emb_dim, name='embedding'),\n",
        "                        tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "                        tf.keras.layers.Dropout(dropout_rate),  \n",
        "                        tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "                        tf.keras.layers.Dropout(dropout_rate),\n",
        "                        *[tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(i, return_sequences=True, name=f'LSTM_{i}')) for j in range(lstm_layers-1) for i in lstm_units],\n",
        "                        ])\n",
        "    for i in range(len(self.seq_layers)):\n",
        "      vars(self)[f'SEQ_LAYER_{i}'] = self.seq_layers[i]\n",
        "\n",
        "    self.ph_layers = [\n",
        "                      tf.keras.layers.Dense(512, name='dense1_ph', activation='relu'),\n",
        "                      tf.keras.layers.Dropout(dropout_rate), \n",
        "                      tf.keras.layers.Dense(256, name='dense2_ph', activation='relu'),\n",
        "                      tf.keras.layers.Dropout(dropout_rate), \n",
        "                      tf.keras.layers.Dense(128, name='dense3_ph', activation='relu'),\n",
        "                      # tf.keras.layers.Dropout(dropout_rate),  \n",
        "                      # tf.keras.layers.Dense(1, name='output_ph')\n",
        "                      ]\n",
        "\n",
        "    self.lambda_layer = tf.keras.layers.Lambda(function=concat, name='lambda_layer')\n",
        "    self.flatten = tf.keras.layers.Flatten(name='flatten')\n",
        "    self.dense_combined = tf.keras.layers.Dense(64, activation='relu', name='dense_combined')\n",
        "    self.lambda_helper = tf.keras.layers.Lambda(lambda x: tf.reshape(x, (-1, 64, 1)))\n",
        "    self.lstm_dense = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512))\n",
        "    self.last_dense = tf.keras.layers.Dense(1, name='output')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    seq, ph = inputs\n",
        "    SEQ_LAYER_0 = vars(self)['SEQ_LAYER_0']\n",
        "    x = SEQ_LAYER_0(seq)\n",
        "    for i in range(1, self.lstm_layers):\n",
        "        SEQ_LAYER_i = vars(self)[f'SEQ_LAYER_{i}']\n",
        "        x = SEQ_LAYER_i(x)\n",
        "\n",
        "    # for layer in self.seq_layers:\n",
        "    #   seq = layer(seq)\n",
        "\n",
        "    for layer in self.ph_layers:\n",
        "      ph = layer(ph)\n",
        "\n",
        "    x = self.lambda_layer([x, tf.tile(tf.reshape(ph, (-1, 128, 1)), (1, 1, x.shape[-1]))])\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense_combined(x)\n",
        "    x = self.lambda_helper(x)\n",
        "    # x = self.lstm_dense(x)\n",
        "    x = self.last_dense(x)\n",
        "    return x\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CM324baOeGHf"
      },
      "outputs": [],
      "source": [
        "TRAIN_STEPS = 15\n",
        "PRUNING_INTERVAL_STEPS = 50\n",
        "def objective(trial):    \n",
        "  lstm_layers = trial.suggest_int('lstm_layers', 1, 7)\n",
        "  emb_dim = trial.suggest_int('emb_dim', 256, 1024)\n",
        "  lstm_units = []\n",
        "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-7, 1)\n",
        "  for i in range(lstm_layers):\n",
        "    lstm_units.append(trial.suggest_int(f'lstm_units_l{i}', 16, 512))\n",
        "  dropout_rate = trial.suggest_uniform('dropout_rate', 0, 1)\n",
        "  optimizer = trial.suggest_categorical('optimizer', [Adagrad, Adadelta, Adam, Adamax, Ftrl, Nadam, SGD, RMSprop])\n",
        "  regressor = MyModel(lstm_layers, emb_dim, lstm_units, dropout_rate)\n",
        "  loss_obj = tf.keras.metrics.MeanAbsoluteError(name='loss_obj')\n",
        "  regressor.compile(loss=loss_obj, optimizer=optimizer(learning_rate=learning_rate))\n",
        "  losses, n_train_iter, step = [], len(get_training_dataset(train)), 0\n",
        "  for epoch in range(1):\n",
        "    print(f'Epoch # {epoch} started')\n",
        "    for batch in tqdm(get_training_dataset(train)):\n",
        "      predictions = regressor([batch[0], batch[1]], training=True)\n",
        "      loss = loss_obj(batch[2], predictions)\n",
        "      losses.append(loss)\n",
        "      if step > n_train_iter//2:\n",
        "        intermediate_value = loss\n",
        "        if intermediate_value < best_loss:\n",
        "            raise optuna.TrialPruned()\n",
        "      step += 1\n",
        "      best_loss = min(losses)\n",
        "    print(f'Training Loss {loss:.2f}, Best Loss: {best_loss:.2f}')\n",
        "    for val_batch in tqdm(get_validation_dataset(val)):\n",
        "      predictions = regressor([val_batch[0], val_batch[1]], training=False)\n",
        "      val_loss = loss_obj(val_batch[2], predictions)\n",
        "    print(f'Validation Loss {val_loss:.2f}')\n",
        "  return val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELrn1aBzLs2P"
      },
      "outputs": [],
      "source": [
        "# study = optuna.create_study(direction=\"minimize\")\n",
        "# study.optimize(objective, n_trials=100)\n",
        "# pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "# complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "989AAezAxUYK"
      },
      "outputs": [],
      "source": [
        "# print(\"Best trial:\")\n",
        "# trial = study.best_trial\n",
        "\n",
        "# trial"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in get_training_dataset(train).take(1):\n",
        "  x = Embedding(vocab_size + 1, 2250)(i[0])\n",
        "  x = Conv1D(filters=64, kernel_size=5, activation='relu')(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = Conv1D(filters=64, kernel_size=3, activation='relu')(x)\n",
        "  x = Bidirectional(LSTM(32, return_sequences=True))(x)\n",
        "  x = GlobalAveragePooling1D()(x)\n",
        "  x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rHH8bqAfyf1",
        "outputId": "ad0d1339-2c8d-43c6-9ff1-e13c97e84bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32, 64), dtype=float32, numpy=\n",
              "array([[-0.02516549,  0.02234669, -0.01176251, ..., -0.00622074,\n",
              "         0.01290033, -0.0035805 ],\n",
              "       [-0.02597752,  0.02234085, -0.01197631, ..., -0.00715735,\n",
              "         0.01415426, -0.00363639],\n",
              "       [-0.02397574,  0.02240723, -0.01191361, ..., -0.00545969,\n",
              "         0.01129605, -0.00363683],\n",
              "       ...,\n",
              "       [-0.02587538,  0.02240031, -0.01197818, ..., -0.00692242,\n",
              "         0.01371748, -0.00369377],\n",
              "       [-0.02433575,  0.0215901 , -0.01156334, ..., -0.0053507 ,\n",
              "         0.01174129, -0.0033725 ],\n",
              "       [-0.02395778,  0.02214839, -0.01166494, ..., -0.00559264,\n",
              "         0.01149958, -0.00318253]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6JB2_gWxUZW"
      },
      "outputs": [],
      "source": [
        "from unicodedata import bidirectional\n",
        "# %pdb\n",
        "class MyModel2(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel2, self).__init__()\n",
        "    self.emb_dim, self.dropout_rate = 64, 0.2\n",
        "    self.input_ph = tf.keras.layers.Input((1,))\n",
        "    self.input_seq = tf.keras.layers.Input((64, ))\n",
        "    self.seq_layers =  [] \n",
        "    self.seq_layers.extend([\n",
        "                        Embedding(vocab_size + 1, self.emb_dim, name='embedding'),\n",
        "                        # Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
        "                        # Dropout(self.dropout_rate),  \n",
        "                        Bidirectional(LSTM(128, return_sequences=True, name='LSTM_5')),\n",
        "                        Dropout(self.dropout_rate),\n",
        "                        Bidirectional(LSTM(32, return_sequences=True, name='LSTM_7')),\n",
        "                        Dropout(self.dropout_rate),\n",
        "                        Bidirectional(LSTM(16, return_sequences=True)),\n",
        "                        Bidirectional(LSTM(8)),\n",
        "                        # GlobalAveragePooling1D(),\n",
        "                        ])\n",
        "\n",
        "    self.ph_layers = [ \n",
        "                      tf.keras.layers.Dense(32, name='dense3_ph', activation='relu'),\n",
        "                      tf.keras.layers.Dropout(self.dropout_rate),  \n",
        "                      tf.keras.layers.Dense(1, name='output_ph')\n",
        "                      ]\n",
        "\n",
        "    self.lambda_layer = Lambda(function=concat, name='lambda_layer')\n",
        "    self.flatten = Flatten(name='flatten')\n",
        "    self.dense_combined = Dense(32, activation='relu', name='dense_combined')\n",
        "    self.lambda_helper = Lambda(lambda x: tf.reshape(x, (-1, 32, 1)))\n",
        "    self.last_dense = Dense(1, activation='relu', name='output')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    seq, ph = inputs\n",
        "\n",
        "    for layer in self.seq_layers:\n",
        "      seq = layer(seq)\n",
        "\n",
        "    for layer in self.ph_layers:\n",
        "      ph = layer(ph)\n",
        "\n",
        "    x = self.lambda_layer([seq, ph])\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense_combined(x)\n",
        "    x = self.lambda_helper(x)\n",
        "    x = self.last_dense(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel2()\n",
        "adam, sgd, rms = Adam(1e-3), SGD(1e-4), RMSprop(1e-4)\n",
        "loss_fn = MeanAbsoluteError()\n",
        "model.compile(optimizer=adam, loss=loss_fn)"
      ],
      "metadata": {
        "id": "-SIehsNSO7cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %pdb\n",
        "for epoch in range(10):\n",
        "  print(f'Epoch # {epoch} started')\n",
        "  for batch in tqdm(get_training_dataset(train)):\n",
        "    predictions = model([batch[0], batch[1]], training=True)\n",
        "    loss = loss_fn(batch[2], predictions)\n",
        "    # print(tf.math.reduce_mean(loss))\n",
        "  for val_batch in tqdm(get_validation_dataset(val)):\n",
        "    predictions = model([val_batch[0], val_batch[1]], training=False)\n",
        "    val_loss = loss_fn(val_batch[2], predictions)\n",
        "  print(f'Validation Loss {tf.math.reduce_mean(val_loss):.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu3JScvqPPH2",
        "outputId": "55d31250-7424-4433-86ee-6da4aac6b5af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch # 0 started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 785/785 [03:22<00:00,  3.89it/s]\n",
            "100%|██████████| 197/197 [00:40<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss 46.71\n",
            "Epoch # 1 started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 785/785 [02:26<00:00,  5.36it/s]\n",
            "100%|██████████| 197/197 [00:40<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss 48.14\n",
            "Epoch # 2 started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 785/785 [03:22<00:00,  3.89it/s]\n",
            "100%|██████████| 197/197 [00:40<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss 56.82\n",
            "Epoch # 3 started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 785/785 [02:26<00:00,  5.36it/s]\n",
            "100%|██████████| 197/197 [00:36<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss 47.83\n",
            "Epoch # 4 started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 785/785 [03:22<00:00,  3.89it/s]\n",
            "100%|██████████| 197/197 [00:36<00:00,  5.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss 59.77\n",
            "Epoch # 5 started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 785/785 [03:22<00:00,  3.89it/s]\n",
            "100%|██████████| 197/197 [00:36<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss 47.59\n",
            "Epoch # 6 started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 785/785 [02:25<00:00,  5.41it/s]\n",
            "100%|██████████| 197/197 [00:35<00:00,  5.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss 45.17\n",
            "Epoch # 7 started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 785/785 [02:24<00:00,  5.42it/s]\n",
            "100%|██████████| 197/197 [00:35<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss 52.61\n",
            "Epoch # 8 started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 785/785 [03:22<00:00,  3.89it/s]\n",
            "100%|██████████| 197/197 [00:40<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss 54.11\n",
            "Epoch # 9 started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 785/785 [03:22<00:00,  3.89it/s]\n",
            "100%|██████████| 197/197 [00:40<00:00,  4.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss 53.16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nsjrc-2UPnM0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('defaultenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c1d7054af77ba560f87b9c7884b1cfb54f44503b6d572a00787a3ee06861aa5a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}